{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOAkztUVjl0mjq+GTCmCRxI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sak2004/Neural_Network_From_Scratch/blob/main/Neural_Network_UsingNumpy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "P1uZEWBh-0tY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creation two layers of 3 neurons and inputing 4 variables"
      ],
      "metadata": {
        "id": "3FBgxXwiM-vu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "inputs = [[1, 2, 3, 2.5],   # 4 inputs to neuron\n",
        "          [2., 5., -1., 2],\n",
        "          [-1.5, 2.7, 3.3, -0.8]]\n",
        "\n",
        "weights = [[0.2, 0.8, -0.5, 1],\n",
        "           [0.5, -0.91, 0.26, -0.5],    # weights for each input to 3 neuron\n",
        "           [-0.26, -0.27, 0.17, 0.87]]\n",
        "\n",
        "biases = [2, 3, 0.5] # biases for each neuron\n",
        "\n",
        "weights2 = [[0.1, -0.14, 0.5],\n",
        "            [-0.5, 0.12, -0.33],\n",
        "            [-0.44, 0.73, -0.13]]\n",
        "\n",
        "biases2 = [-1, 2, -0.5]\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "inputs_array = np.array(inputs)\n",
        "weights_array = np.array(weights)\n",
        "biases_array = np.array(biases)\n",
        "weights2_array = np.array(weights2)\n",
        "biases2_array = np.array(biases2)\n",
        "\n",
        "# Calculate the output of the first layer\n",
        "layer1_outputs = np.dot(inputs_array, weights_array.T) + biases_array\n",
        "\n",
        "# Calculate the output of the second layer\n",
        "layer2_outputs = np.dot(layer1_outputs, weights2_array.T) + biases2_array\n",
        "\n",
        "print(layer2_outputs) # here input value passed though two layers using simple matrix multiplication between input and transpose of weights and addition of biases"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaq3j6fvLgNt",
        "outputId": "74dbfc4c-00a0-486e-ee5e-6a997292d085"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.50310004 -1.04184985 -2.03874993]\n",
            " [ 0.24339998 -2.73320007 -5.76329994]\n",
            " [-0.99314     1.41254002 -0.35655001]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating Dense Function Class\n"
      ],
      "metadata": {
        "id": "o1SVvho1TUKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Layer_Dense:\n",
        "  def __init__(self,n_inputs,n_neurons):\n",
        "    self.weights = 0.10 * np.random.randn(n_inputs,n_neurons)\n",
        "    self.biases = np.zeros((1,n_neurons))\n",
        "  def forward_pass(self,inputs):\n",
        "    self.output = np.dot(inputs,self.weights) + self.biases\n",
        "\n",
        "dense1 = Layer_Dense(2,3)\n",
        "dense1.forward_pass(X)\n",
        "print(dense1.output[:5])\n",
        "\n"
      ],
      "metadata": {
        "id": "Wg-Rn9ytM7mO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b6029f0-6683-42a7-eeff-571d86c232f7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.          0.          0.        ]\n",
            " [-0.00104752  0.00113954 -0.00047984]\n",
            " [-0.00274148  0.00317292 -0.00086922]\n",
            " [-0.00421884  0.00526663 -0.00055913]\n",
            " [-0.00577077  0.00714014 -0.0008943 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Implementing Activation Layers\n"
      ],
      "metadata": {
        "id": "u6tLXg3gVg5C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing ReLu Class as Activation Layer"
      ],
      "metadata": {
        "id": "9RPiBSoNWEMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Activation_Relu :\n",
        "  def forward_pass(self,inputs):\n",
        "    self.output = np.maximum(0,inputs)\n"
      ],
      "metadata": {
        "id": "dPUPwad4Vnx3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing Softmax Activation Function for predicted Outputs"
      ],
      "metadata": {
        "id": "2Lf-wUo4WY5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Activation_Softmax:\n",
        "  def forward_pass(self,inputs):\n",
        "\n",
        "    exp_value = np.exp(inputs - np.max(inputs,axis=1,keepdims=True))\n",
        "    probabilities = exp_value / np.sum(exp_value,axis=1,keepdims=True)\n",
        "    self.output = probabilities"
      ],
      "metadata": {
        "id": "o_hAn5XTVvac"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing Neural Network on Spiral Dataset"
      ],
      "metadata": {
        "id": "B5aDAGcmZbDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nnfs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmJ1jPVMaGIz",
        "outputId": "bdf5d5a2-5f39-43ab-f8e9-3990bab03d71"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nnfs in /usr/local/lib/python3.12/dist-packages (0.5.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from nnfs) (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "import nnfs #using nnfs to generate a spiral data set\n",
        "from nnfs.datasets import spiral_data\n",
        "nnfs.init()\n",
        "X,y =spiral_data(samples=100,classes= 3)"
      ],
      "metadata": {
        "id": "wA3I75ZLZsmo"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dense1=Layer_Dense(2,3)\n",
        "activation1=Activation_Relu()\n",
        "dense2=Layer_Dense(3,3)\n",
        "activation2=Activation_Softmax()\n",
        "dense1.forward_pass(X)\n",
        "activation1.forward_pass(dense1.output)\n",
        "dense2.forward_pass(activation1.output)\n",
        "activation2.forward_pass(dense2.output)\n",
        "print(activation2.output[:5])  # here output for radom weights selection gave really poor out so we need to find a way optimaize the weights selection ;(\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGp4k6SFaN3B",
        "outputId": "6cc6ddb6-75b8-4ea2-d905-6fc9037e7681"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.33333334 0.33333334 0.33333334]\n",
            " [0.33331734 0.3333183  0.33336434]\n",
            " [0.3332888  0.33329153 0.33341965]\n",
            " [0.33325943 0.33326396 0.33347666]\n",
            " [0.33323312 0.33323926 0.33352762]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing Cross Entropy Catregorical loss class"
      ],
      "metadata": {
        "id": "f-bgzUZt93_6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing common loss function"
      ],
      "metadata": {
        "id": "FLdgzpzo-4YA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Loss:\n",
        "  def calculate(self,output,y):\n",
        "    sample_losses = self.forward(output,y)\n",
        "    data_loss = np.mean(sample_losses)\n",
        "    return data_loss"
      ],
      "metadata": {
        "id": "hPlLRD1C-8nd"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Loss_CategoricalCrossentropy(Loss):\n",
        "  def forward(self,y_pred,y_true):\n",
        "    samples = len(y_pred)\n",
        "    y_prep_clipped = np.clip(y_pred,1e-7,1-1e-7)\n",
        "\n",
        "    if len(y_true.shape) == 1:\n",
        "      correct_confidences = y_prep_clipped[range(samples),y_true]\n",
        "    elif len(y_true.shape)==2:\n",
        "      correct_confidences= np.sum(y_prep_clipped*y_true,axis=1)\n",
        "    negative_log_likelihood = -np.log(correct_confidences)\n",
        "    return negative_log_likelihood"
      ],
      "metadata": {
        "id": "OUunlid7-_Ae"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X,y =spiral_data(samples=100,classes= 3)\n",
        "\n",
        "dense1=Layer_Dense(2,3)\n",
        "activation1=Activation_Relu()\n",
        "dense2=Layer_Dense(3,3)\n",
        "activation2=Activation_Softmax()\n",
        "loss_function = Loss_CategoricalCrossentropy()\n",
        "dense1.forward_pass(X)\n",
        "activation1.forward_pass(dense1.output)\n",
        "dense2.forward_pass(activation1.output)\n",
        "activation2.forward_pass(dense2.output)\n",
        "print(activation2.output[:5])\n",
        "\n",
        "loss = loss_function.calculate(activation2.output,y)\n",
        "print(\"Loss:\",loss)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xL7Jco_7A8v-",
        "outputId": "e6894575-d2af-4b0b-ab25-779f0aacc547"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]]\n",
            "Loss: 1.0999713\n"
          ]
        }
      ]
    }
  ]
}